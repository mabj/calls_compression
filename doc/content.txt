[Context]
I have been working on a solution for compressing files containing trace of API calls coming out of a sandbox. This file holds events generated by a monitor module (*DLL*) which is injected into processes and hooks a pre-configured set of API calls. This file describes the interaction between a process and the Operating System (*Behavioral* data). 
[OK]

This file can grow in size (memory demanding), complexity (CPU demanding) and noise (difficult to read) depending on two main factors: 
    - how long an analysed process is executed; and 
    - presence of specific control flow structures (e.g. loops).
[OK]

This article proposes two methods for detecting looping patterns. The outcome of these methods is further used for compressing the above described sandbox log files without changing its structure.
[OK]

Detecting loops by observing its execution is a complex task since not always a well defined pattern emerges. This article names "repeated element" and "body" a content inside a loop statement. This element can be composed by a single call (repeated multiple times), multiple sequential calls and nested calls; its execution output can also be affected by control flow elements such as conditionals.
[OK]

Repeated elements can also be affected by pseudo-random number generators. In this case, the generated log file presents more complex patterns. Algorithms described in this article do not consider this less common scenario and target detection of more general looping patterns.
[OK]

[Examples and Samples]

Python-like scripts are presented along this article to ilustrate the proposed techniques. The following pseudo-code describes a loop executing calls to functions labeled from 1 ("_call_001") to 5 ("_call_005"):

### [SNIP_01]
1    _call_001()
2    for i in range(0 to 4):
3        _call_002()
4        _call_003()
5        if i is even:
6            _call_004()
7    _call_005()
###

The body of the loop statement (line "2") executes "5" times functions "002" and "003" and executes function "004" every time "i" is even. The behavioral log produced by executing the script above is:

### [OUTPUT_SNIP_01]
1     (line1, c_001)
2     (line3, c_002)
3     (line4, c_003)
4     (line6, c_004)    # i = 0
5     (line3, c_002)
6     (line4, c_003)    # i = 1
7     (line3, c_002)
8     (line4, c_003)
9     (line6, c_004)    # i = 2
10    (line3, c_002)
11    (line4, c_003)    # i = 3
12    (line3, c_002)
13    (line4, c_003)
14    (line6, c_004)    # i = 4
15    (line7, c_005)
###
[OK]

An entry containing a position ("line[x]") and an identifier ("c_00[x]") to a call is added to this output everytime a function is executed. Taking a closer look to this log it is possible to observe repetition pattern from lines 2 to 14. These lines present an execution trace of the loop and could be compressed to the following form:

### [OUTPUT_SNIP_01 COMPRESSED]
1    (line1, c_001)
2    (line3, c_002) x5
3    (line4, c_003) x5
4    (line6, c_004) x3
5    (line7, c_005)
###
[OK]

The compressed output has the following propeties:

    - reduced the original file in three times (66% compression rate);
    - preserved the original order that functions are executed inside the loop; and 
    - easier to read;

Next sessions describes two methods to reach the compression exemplified above. The first uses Sufix Trees [REFERENCE] and non-overlapping Longest Repeated Subset [REFERENCE] and the second one uses N-Gram [REFERENCE].
[OK]

[Method 001: Non-Overlapping Longest Repeated Substring]

The first method consists in using Non-Overlaping Longest Repeated Substring which is a well stablished algorithm of complexity O(n^2) [REFERENCE]. This algorithm will find repeated subsets of tokens which are composed by a checksum of data extracted from each entry in the log - this will be the Identifier of an API call setup.

The following pseudo-code shows a simplified representation of this algorithm.

### [SNIP_02]
1    data = []
2    for entry in log_entries:
3        c = checksum(entry)
4        data.append(c)
5
6    map = {}
7    lrs = True
8    while lrs:
9         lrs = calculate_lrs(data)
10        while lrs:
11            lrs = calculate_lrs(lrs)
12
13        data, map = replace_lrs(data, map, lrs)
14        data = delete_repeated(data)
15
16    data = transform_back(data, map)
###

The first part of this code (lines 1 to 4) creates a vector named "data" containing checksums calculated from each log entry (we call these checksums "primitive" tokens). The algorithm runs in a loop until "calculate_lrs()" returns an empty value. "replace_lrs()" function replaces the instances of "lrs" vector within "data" with an unique token. This token (and its respective LRS vector) is saved in a translation map (variable named "map"). The next step is reducing repetitive tokens to a single token and its count (how many times it repeats). At the end, tokens used to represent LRS vectors are translated back to "primitive" tokens by using the translation map (line 15). 

The second "while" listed in line 10, helps detecting repetition patterns inside LRS vectors. Next code snip describes a use case of this second "while" loop.

### [SNIP_02_01] 
1    data = [0, 1, 2, 2, 2, 2, 0, 1, 2, 2, 2, 2]
2    lrs = calculate_lrs(data)  # [0, 1, 2, 2, 2, 2]  <= First While
3       
4    while lst:
5        lrs = calculate_lrs()
6    # [2, 2]                                         <= Second while
###

In this case the vector "[2,2]" would be replaced with an unique token in the original data vector and the main loop moves to its next iteration using this new transformed vector. 

The final output for the input vector above would be "[(0, 2), (1, 2), (2, 8)]". Where the first element is the token and the second element is the amount of repetitions. 

The code below is a Python implementation of the logic described above.  

### [SNIP_03]
    import os
    import sys
    import logging
    import hashlib
    import numpy
    from random import randint

    from itertools import groupby, chain

    logger = logging.getLogger(__name__)
    LOGGING_FORMAT = '[%(levelname)s] [%(asctime)s] %(funcName)s(): %(message)s'
    logging.basicConfig(level=logging.INFO, format=LOGGING_FORMAT)

    def FindRepeatedSubstring(data): 
        n = len(data)
        LCSRe = numpy.zeros((n+1,n+1), dtype=int)
        output = set()
        matches = set()

        for i in range(1, n + 1): 
            for j in range(i + 1, n + 1): 
                if (data[i - 1] == data[j - 1] and
                    LCSRe[i - 1][j - 1] < (j - i)): 
                    LCSRe[i][j] = LCSRe[i - 1][j - 1] + 1

                    if(LCSRe[i][j] > 0):
                        _len = LCSRe[i][j]
                        if (_len, i) not in matches:
                            output.add(data[i - _len:i])
                        matches.add((_len, i))
                else: 
                    LCSRe[i][j] = 0

        return output

    def count_consecutive(main_vector, base_vector):
        label = -1
        main_vector = replace_subvector_02(main_vector, list(base_vector), label)
        return len(list(filter(lambda x: x == label, main_vector)))

    def replace_subvector_02(data, base_vector, label):
        output = []
        u = 0
        for i in range(len(data)):
            if data[i] != base_vector[u]:
                u = 0

            if data[i] == base_vector[u]:
                u += 1 
                if u == len(base_vector):
                    output.append(label)
                    u = 0
                continue
            u = 0

            tmp = []
            output.append(data[i])

        return output

    def find_base_vectors(main_vector):
        selected_vectors = FindRepeatedSubstring(tuple(main_vector))

        _max = []
        for v in selected_vectors:
            mc = count_consecutive(main_vector, v)
            if mc > 0:
                _max.append([mc, v])

        logger.debug('[+] Could find %d base vectors with consecutive matches.', len(_max))

        if not _max:
            return []

        _max.sort(key=lambda x: x[0], reverse=True)
        logger.debug('[+] First vector has %d elements and %d occurences', len(_max[0][1]), _max[0][0])

        main_vectors = [x[1] for x in _max]
        return main_vectors

    def replace_subvector(data, base_vector, label):
        output = []
        tmp = []
        u = 0
        for i in range(len(data)):
            if data[i][0] != base_vector[u]:
                u = 0

            if data[i][0] == base_vector[u]:
                u += 1 
                tmp.append(data[i][1])
                if u == len(base_vector):
                    output.append([label, tmp])
                    tmp = []
                    u = 0
                continue
            u = 0
            tmp = []
            output.append(data[i])

        return output

    # Finds biggest repeated subvector and replace it with a token
    def __transform_001(data):
        indexes = [ c[0] for c in data ]
        base_vectors = find_base_vectors(indexes)

        if len(base_vectors) <= 1:
            return data, False

        for base_vector in base_vectors:
            label = (sum(base_vector) + randint(1, 20)) * len(data)
            data = replace_subvector(data, base_vector, label)

        return data, True

    def __update_repeated(element, e):
        if type(element) is not list:
            element['repeated'] += e['repeated']
            return element

        for i in range(len(element)):
            element[i] = __update_repeated(element[i], e[i])

        return element

    # Removes repeated consecutive elements in a vector
    def __transform_002(data):
        output = []
        last_element = [-1, None]
        for d in data:
            if last_element[0] == d[0]:
                output[-1][1] = __update_repeated(last_element[1], d[1])
                continue

            output.append(d)
            last_element = d

        return output

    def __checksum(call):
        h = hashlib.md5(call.encode('utf-8'))
        return h.hexdigest()

    def __expand_element(e):
        if type(e) is not list:
            return [e]

        output = []
        for x in e:
            output += __expand_element(x)

        return output

    def __compress_calls(data=[]):
        # Collect calls and create indexes
        calls = []
        unique_checksums = []
        for c in data:
            csum = __checksum(c)
            if csum not in unique_checksums:
                unique_checksums.append(csum)
            calls.append([unique_checksums.index(csum), {'label': c, 'repeated': 1} ])

        logger.debug('[+] Starting transformations ... ')
        while True:
            # Transformation 001
            (calls, has_changes) = __transform_001(calls)
            if not has_changes:
                break
            # Transformation 002
            calls = __transform_002(calls)

        logger.debug('[+] Amount of API calls after transformations: %d', len(calls))
        logger.debug('[+] Expanding list ... ')

        calls_expanded = []
        for c in calls:
            calls_expanded += __expand_element(c[1])

        return calls_expanded

    def main(argv):
        calls = [
            'call_0', 'call_1', 'call_2', 'call_2', 'call_2', 'call_2', \
            'call_0', 'call_1', 'call_2', 'call_2', 'call_2', 'call_2'
        ]

        print(calls)
        compressed_calls = __compress_calls(calls)
        print(compressed_calls)

    if __name__ == "__main__": 
        sys.exit(main(sys.argv))
###

The output to the example above is:

### [OUTPUT_SNIP_03]
1    ['call_0', 'call_1', 'call_2', 'call_2', 'call_2', 'call_2', 'call_0', 'call_1', 'call_2', 'call_2', 'call_2', 'call_2']
2    [{'label': 'call_0', 'repeated': 2}, {'label': 'call_1', 'repeated': 2}, {'label': 'call_2', 'repeated': 8}]
###

Initial log entries (line 1) were compressed (line 2) and the information of repetition was appended. This method is example was just a simulation of how a possible library implementing this method could be applied in a real scenario for compressing behavioural logs.

This method has an upside related to its capability in compressing logs containing entries related to loops of any size (repeated element or body). However it has also few really strong drawbacks such as: 

    - memory comsuption: as all log entries have to be loaded in memory in order to run the "FindRepeatedSubstring" function;
    - processing time: the complexity is not that bad O(n^2) but the algorithm is called many times and this impacts its efficiency.

This method can be used in a scenario with many small/medium size log files and lossless looping compression as a hard-requirement. 

These limitations pushed this research to experimenting other approaches in order to get better efficiency. Since the log files are relativelly big and loops usually perform small and concise block of operations so the full coverage (to all sizes of looping body) is not really necessary. 

[Method 002: N-Grams]






[REFERENCES]

[1] https://en.wikipedia.org/wiki/Longest_repeated_substring_problem
[2] https://www.geeksforgeeks.org/longest-repeating-and-non-overlapping-substring/
[3] https://lijiyao111.gitbooks.io/algorithm-and-data-structure/Array_DP/LongestRepeatingSubstring.html


